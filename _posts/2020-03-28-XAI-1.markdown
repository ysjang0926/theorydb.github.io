---
layout: post
title:  "Introduction to Explainable AI - (1)"
subtitle:   "HackerRank: SQL Basic Select"
categories: data
tags: dl
comments: true
---

* 최근 [XAI 설명 가능한 인공지능, 인공지능을 해부하다](https://wikibook.co.kr/xai/) 책이 출간되어 주문한 기념으로, Explainable AI(설명 가능한 인공지능)를 소개하는 글을 쓰게 되었습니다.
* [Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models](https://arxiv.org/abs/1708.08296) 논문을 간단히 정리하고 조금의 추가적인 내용을 덧붙인 글입니다.

-------

인공지능(Artificial Intelligence, AI) 기술은 사용자에게 추천과 예측 등의 정보를 제공하며, 거의 모든 분야에 다양한 용도로 사용되고 있습니다. 하지만 알고리즘의 복잡성으로 인해 어떻게 이런 결정을 했는지 파악할 수 없기에, 이를 AI의 "블랙박스"라고 부릅니다. 도출한 최종 결과의 근거와 도출 과정의 타당성을 제공하지 못하는 이슈가 존재하고, 이를 해결하고자 등장한 것이 “**설명가능한 AI(Explainable AI, XAI)**”입니다. <br>

## 1. Introduction
본 논문의 경우 2017년에 쓰여진 논문이다보니, 2016년에 있었던 이세돌 선수와 알파고의 대결을 예시로 들고 있습니다.
![이세돌](https://user-images.githubusercontent.com/54492747/77823456-aba74a80-713e-11ea-9f34-af3fc40e4c3a.jpg)
이세돌 선수와의 대결에서, 알파고는 '해당 지점에 왜 바둑돌을 두었을까'에 대한 의문에 대해 전혀 투명하게 정보가 제공되지 않았다는 점에서 문제가 제기되었습니다. 알파고 접근법 성공에 대한 논리적 근거 부족성으로 인해, 알파고의 결정 과정을 이해하거나 검증할 수 없기 때문입니다.

입력 데이터(input data)에 대해 어떤 정보가 모델의 결과에 도달했는지 명확하지 않은 모델을 "블랙박스(black-box) 모델"라고 합니다. 블랙박스라고 하는 이유는 데이터를 사용해 특정 결론에 이르는 과정이 투명하지 않고, 이와 같은 모호성으로 인해 시스템의 특정 행동 방식이나 그 이유를 알기가 어렵다는 데 있습니다. 그렇기 때문에 많은 분야에서 AI시스템의 의사결정 과정을 이해하고 검증하는 것이 불가능하다는 것은 큰 단점입니다.
1. 의료
	* 생명이 걸린 문제이기 때문에  블랙박스 모델을 전적으로 신뢰하는 것은 굉장히 위험하고 무책임한 일입니다. 그래서 무엇보다 전문가가 적절하게 개입하고 확인이 필요한 영역입니다.
2. 자율주행
	* 한번의 잘못된 예측이 매우 많은 비용을 초래하기 때문에 올바른 특징에 기반한 모델이 보장되어야 합니다.
3. 금융
	* 고객의 개인적인 정보를 다루기 때문에, 신뢰성과 공정성을 보장할 수 있어야 합니다.

딥러닝 모델의 경우 우수한 성능을 보이더라도, 모델의 복잡한 비선형 구조로 인해 데이터 차원에서 그 요인을 파악하는 것은 어렵습니다. 즉, 비선형적인 구조가 지속적으로 나옴에 따라 정확한 예측에 도달하는 것에 대한 정보가 되지 않음으로 투명성 부족에 대한 문제가 야기됩니다. 이러한 문제를 해결하기 위해 딥러닝 모델이 판단한 근거와 판단 과정의 타당성에 대한 설명을 하고자 등장한 것이 바로 "설명가능한 AI"입니다.

<br> 

### 🔮 설명가능한 AI
* 사용자가 인공지능 시스템의 동작과 최종 결과를 이해하고 올바르게 해석하여 결과물이 생성되는 과정을 설명 가능하도록 해주는 기술
* 모델이 내부적으로 작동하는 방식을 이해할 수 있게 해줌
* 예를 들어 AI 시스템이 고양이 이미지를 분류할 경우, 기존 시스템은 입력된 이미지의 고양이 여부만을 도출하지만 XAI는 고양이 여부를 도출하고 털과 콧수염, 발톱 등으로 고양이로 판단한 근거까지 사용자에게 제공함
![xai](https://user-images.githubusercontent.com/54492747/77825570-72c2a200-714d-11ea-9cb7-653a69acf785.PNG)
(사진 출처: [https://www.darpa.mil/attachments/DARPA-BAA-16-53.pdf](https://www.darpa.mil/attachments/DARPA-BAA-16-53.pdf))  

그리하여 머신러닝(ML)과 인공지능(AI)에 대한 설명성의 필요성에 대한 인식을 높이는 것을 본 논문의 주요 목표로 삼고 있습니다. 
 
 <br>
 
 ## 2. Why do we need Explainable AI?

AI의 설명가능성을 찬성하는 총 4가지의 주장이 있으며, 아래와 같습니다.
* 시스템 검증
* 시스템 개선
* 시스템으로부터의 학습
* 법규 준수

**1. 시스템 검증 (Verification of the system)** <br>
기본적으로 블랙박스 시스템을 전적으로 신뢰하면 안됩니다. 예를 들어, 의료 분야에서는 의료 전문가들에 의해 해석되고 검증될 수 있는 모델을 사용하는 것이 절대적으로 필요합니다.
예시 : 사람의 폐렴 위험을 예측하도록 훈련된 AI 시스템 ([논문](https://www.aminer.cn/pub/5736973b6e3b12023e62b254/intelligible-models-for-healthcare-predicting-pneumonia-risk-and-hospital-day-readmission))
	* 사전정보) 천식 또는 심장병 환자들은 전문가에 의해 엄격한 관리를 받고 환자들의 민감성이 증가함에 따라 사망 확률이 낮습니다.
	* 위의 사전 정보를 모르고 AI 시스템은 건강한 사람과 대조적으로 체계적으로 편향된 데이터(biased data)를 학습하게 되며, 사망확률이 건강한 사람이 천식 환자보다 더 높다는 잘못된 결론에 도달하게 됩니다.
![부작용](https://user-images.githubusercontent.com/54492747/77824131-98e34480-7143-11ea-8a38-3df91342c838.PNG)

**2. 시스템 개선 (Improvement of the system)** <br>
AI 시스템 개선의 가장 첫번째 단계는 취약점을 이해하는 것입니다. 즉, AI 시스템에 있어서 어디에 결함이 있는지 분석하는 것이 중요합니다. 만약 모델이 수행하는 작업과 예측에 도달하는 이유를 이해한다면, 편향된 데이터를 덜어내는 것이 더 용이해지고 모델을 개선하기 더 쉬워질 것입니다.

**3. 시스템으로부터의 학습 (Learning from the system)** <br>
AI 시스템은 수백만개의 예시로 훈련되며, 이를 통해 사람이 접근할 수 없는 데이터에서 패턴을 관찰할 수 있습니다. XAI를 사용하게 된다면, 새로운 관점을 얻기 위해 AI 시스템에서 지식을 추출할 수 있을 것입니다.
과학의 경우, 과학자들은 블랙박스 모델로 어떠한 양을 예측하는 것보다 자연의 숨겨진 법칙을 식별할 수 있다는 것에 관심이 있습니다. 따라서 XAI는 이러한 과학의 영역에서 더욱 유용하게 사용될 것입니다.

**4. 법규 준수 (Compliance of legislation)** <br>
AI 시스템은 법적 질문과 그 결정의 이유에 대한 만족스러운 답변을 가져야 합니다. 투명성 있는 답변을 하기 위해 XAI의 필요성이 증가하게 됩니다. 예를 들어 은행에서 대출을 한다고 할 경우, 개인의 권리 측면에서도 은행의 대출 거부자에게 어떠한 이유로 이러한 결정을 하였는지 설명하는 것이 가능해야하는 필요성이 강조됩니다.
현재 유럽연합(EU)은 '설명권(right to explanation)'에 대한 새로운 규정을 적용함으로써, 사용자는 자신에 대한 알고리즘적 결정에 대한 설명을 요청할 수 있게 되었습니다.
